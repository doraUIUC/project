{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyMyOd3XY4z6mzeAK4IXgjw5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fV5w0kmq8kN","executionInfo":{"status":"ok","timestamp":1683686995798,"user_tz":300,"elapsed":11932,"user":{"displayName":"Darkhan Baizhan","userId":"08245259182187097777"}},"outputId":"c226f2a4-422a-4667-a44c-c52a2db7bb26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/MyDrive/CS498_project/PSMNet\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/MyDrive/CS498_project/PSMNet/"]},{"cell_type":"code","source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.utils.data\n","import torch.nn.functional as F\n","import math\n","from models.submodule import *\n","from models.stackhourglass import hourglass\n","\n","\n","import os\n","import random\n","import torch\n","from torchvision import transforms as transforms\n","import numpy as np\n","import time\n","import cv2\n","from PIL import Image\n","from torchvision.models import resnet18\n","import pdb\n","\n","from tqdm import tqdm"],"metadata":{"id":"F5Unj3hYHQQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_path = '/content/gdrive/MyDrive/CS498_project/PSMNet/'"],"metadata":{"id":"Pla1NCqJHQ23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.nn.functional as F\n","import skimage\n","import skimage.io\n","import skimage.transform\n","import numpy as np\n","import time\n","import math\n","import copy"],"metadata":{"id":"PyImjaFo_eEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision.transforms as transforms\n","import random\n","from PIL import Image, ImageOps\n","import numpy as np\n","from dataloader import preprocess\n","\n","from pathlib import Path\n","\n","\n","class KITTI(torch.utils.data.Dataset):\n","    def __init__(self, data_path, mode):\n","\n","\n","        left_fold  = 'colored_0/'\n","        right_fold = 'colored_1/'\n","        disp   = 'disp_occ/'\n","\n","        self.mode = mode\n","        image_paths = np.array([[str(img),str(img).replace(left_fold, right_fold),str(img).replace(left_fold, disp)] for img in Path(data_path+left_fold).glob(\"*_10.*\")])\n","        \n","        np.random.shuffle(image_paths)\n","\n","\n","        split_point = int(len(image_paths)*0.85)\n","\n","        data_split = {'train':image_paths[:split_point],\n","                      'val': image_paths[split_point:-5],\n","                      'test': image_paths[-5:]}\n","\n","\n","\n","        self.left  =  data_split[self.mode][:,0]\n","        self.right = data_split[self.mode][:,1]\n","        self.disp_true = data_split[self.mode][:,2]\n","\n","\n","    def __getitem__(self, index):\n","\n","        left_path  = self.left[index]\n","        right_path = self.right[index]\n","        disp_true_path = self.disp_true[index]\n","\n","        left_img = Image.open(left_path).convert('RGB')\n","        right_img = Image.open(right_path).convert('RGB')\n","        disp_true = Image.open(disp_true_path)\n","\n","   \n","\n","        if self.mode == 'train':  \n","\n","           w, h = left_img.size\n","           th, tw = 256, 512\n"," \n","           x1 = random.randint(0, w - tw)\n","           y1 = random.randint(0, h - th)\n","\n","           left_img = left_img.crop((x1, y1, x1 + tw, y1 + th))\n","           right_img = right_img.crop((x1, y1, x1 + tw, y1 + th))\n","\n","           disp_true = np.ascontiguousarray(disp_true,dtype=np.float32)/256\n","           disp_true = disp_true[y1:y1 + th, x1:x1 + tw]\n","\n","           processed = preprocess.get_transform(augment=False)  \n","           left_img   = processed(left_img)\n","           right_img  = processed(right_img)\n","\n","           return left_img, right_img, disp_true\n","        else:\n","           w, h = left_img.size\n","\n","           left_img = left_img.crop((w-1232, h-368, w, h))\n","           right_img = right_img.crop((w-1232, h-368, w, h))\n","           w1, h1 = left_img.size\n","\n","           disp_true = disp_true.crop((w-1232, h-368, w, h))\n","           disp_true = np.ascontiguousarray(disp_true,dtype=np.float32)/256\n","\n","           processed = preprocess.get_transform(augment=False)  \n","           left_img       = processed(left_img)\n","           right_img      = processed(right_img)\n","\n","           return left_img, right_img, disp_true\n","\n","    def __len__(self):\n","        return len(self.disp_true)"],"metadata":{"id":"zm106SRGZcln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNetFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(ResNetFeatureExtractor, self).__init__()\n","        self.conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1)\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.upsample1 = nn.Upsample(size=(192, 616), mode='bilinear', align_corners=True)\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1, stride=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.upsample2 = nn.Upsample(size=(192, 616), mode='bilinear', align_corners=True)\n","\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1, stride=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.upsample3 = nn.Upsample(size=(96, 308), mode='bilinear', align_corners=True)\n","\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(64, 32, kernel_size=3, padding=1, stride=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(x)\n","        x = self.upsample1(x)\n","        x = self.conv2(x)\n","        x = self.upsample2(x)\n","        x = self.conv3(x)\n","        x = self.upsample3(x)\n","        x = self.conv4(x)\n","        return x"],"metadata":{"id":"01NZJeUSGafj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PSMNet(nn.Module):\n","    def __init__(self, maxdisp, transfer_learning = False):\n","        super(PSMNet, self).__init__()\n","        self.maxdisp = maxdisp\n","\n","        if transfer_learning:\n","          \n","          self.feature_extraction = ResNetFeatureExtractor()\n","\n","        else:\n","          self.feature_extraction = feature_extraction()\n","\n","\n","        self.dres0 = nn.Sequential(convbn_3d(64, 32, 3, 1, 1),\n","                                     nn.ReLU(inplace=True),\n","                                     convbn_3d(32, 32, 3, 1, 1),\n","                                     nn.ReLU(inplace=True))\n","\n","        self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n","                                   nn.ReLU(inplace=True),\n","                                   convbn_3d(32, 32, 3, 1, 1)) \n","\n","        self.dres2 = hourglass(32)\n","\n","        self.dres3 = hourglass(32)\n","\n","        self.dres4 = hourglass(32)\n","\n","        self.classif1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n","\n","        self.classif2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n","\n","        self.classif3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.Conv3d):\n","                n = m.kernel_size[0] * m.kernel_size[1]*m.kernel_size[2] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.bias.data.zero_()\n","\n","\n","    def forward(self, left, right, resnet_L_fea, resnet_R_fea):\n","\n","        if transfer_learning:\n","\n","          refimg_fea = self.feature_extraction(resnet_L_fea)\n","          targetimg_fea = self.feature_extraction(resnet_R_fea)\n","\n","        else:\n","\n","          refimg_fea     = self.feature_extraction(left)\n","          targetimg_fea  = self.feature_extraction(right)\n","\n","        #matching\n","        cost = torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.maxdisp//4,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_().cuda()\n","\n","        for i in range(self.maxdisp//4):\n","            if i > 0 :\n","             cost[:, :refimg_fea.size()[1], i, :,i:]   = refimg_fea[:,:,:,i:]\n","             cost[:, refimg_fea.size()[1]:, i, :,i:] = targetimg_fea[:,:,:,:-i]\n","            else:\n","             cost[:, :refimg_fea.size()[1], i, :,:]   = refimg_fea\n","             cost[:, refimg_fea.size()[1]:, i, :,:]   = targetimg_fea\n","        cost = cost.contiguous()\n","\n","        cost0 = self.dres0(cost)\n","        cost0 = self.dres1(cost0) + cost0\n","\n","        out1, pre1, post1 = self.dres2(cost0, None, None) \n","        out1 = out1+cost0\n","\n","        out2, pre2, post2 = self.dres3(out1, pre1, post1) \n","        out2 = out2+cost0\n","\n","        out3, pre3, post3 = self.dres4(out2, pre1, post2) \n","        out3 = out3+cost0\n","\n","        cost1 = self.classif1(out1)\n","        cost2 = self.classif2(out2) + cost1\n","        cost3 = self.classif3(out3) + cost2\n","\n","        if self.training:\n","            cost1 = F.upsample(cost1, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')\n","            cost2 = F.upsample(cost2, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')\n","\n","            cost1 = torch.squeeze(cost1,1)\n","            pred1 = F.softmax(cost1,dim=1)\n","            pred1 = disparityregression(self.maxdisp)(pred1)\n","\n","            cost2 = torch.squeeze(cost2,1)\n","            pred2 = F.softmax(cost2,dim=1)\n","            pred2 = disparityregression(self.maxdisp)(pred2)\n","\n","        cost3 = F.upsample(cost3, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')\n","        cost3 = torch.squeeze(cost3,1)\n","        pred3 = F.softmax(cost3,dim=1)\n","        #For your information: This formulation 'softmax(c)' learned \"similarity\" \n","        #while 'softmax(-c)' learned 'matching cost' as mentioned in the paper.\n","        #However, 'c' or '-c' do not affect the performance because feature-based cost volume provided flexibility.\n","        pred3 = disparityregression(self.maxdisp)(pred3)\n","\n","        if self.training:\n","            return pred1, pred2, pred3\n","        else:\n","            return pred3"],"metadata":{"id":"eBQ_ulks-bjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_model(maximum_disp,transfer_learning,model_weights):\n","\n","  model = PSMNet(maximum_disp,transfer_learning)\n","  \n","  model = nn.DataParallel(model, device_ids=[0]) #although I fine-tuned on 1 gpu, it is needed since state_dict keys are saved that way\n","\n","  if torch.cuda.is_available():\n","    model.cuda()\n","\n","  pretrained_weights  = torch.load(model_weights)\n","\n","  if transfer_learning:\n","\n","        \n","    psm_weights = model.state_dict()\n","\n","    for key in psm_weights.keys():\n","      if key in pretrained_weights and pretrained_weights[key].shape == psm_weights[key].shape:\n","      \n","        psm_weights[key] = pretrained_weights[key]\n","    \n","    model.load_state_dict(psm_weights)\n","\n","  else:\n","            \n","      model.load_state_dict(pretrained_weights['state_dict'])\n","\n","      \n","  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","  params = sum([np.prod(p.size()) for p in model_parameters])\n","\n","  print(f\"Number of trainable parameters in model: {params}\")\n","  \n","  return model"],"metadata":{"id":"Vi1C0bKArzVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_3px_err(pred_disp, disp_gt):\n","\n","        index = np.argwhere(disp_gt>0)\n","\n","        abs_diff = torch.zeros_like(disp_gt)\n","\n","        abs_diff[index[0][:], index[1][:], index[2][:]] = np.abs(disp_gt[index[0][:], index[1][:], index[2][:]] - pred_disp[index[0][:], index[1][:], index[2][:]])\n","\n","        correct = (abs_diff[index[0][:], index[1][:], index[2][:]] < 3)|(abs_diff[index[0][:], index[1][:], index[2][:]] < disp_gt[index[0][:], index[1][:], index[2][:]]*0.05)      \n","        \n","        torch.cuda.empty_cache()\n","\n","        return 1-(float(torch.sum(correct))/float(len(index[0])))\n","\n","def extract_resnet_features(resnet, imgL,imgR):\n","\n","    if resnet is None:\n","      return None, None\n","    else:\n","      return resnet(imgL).cuda() , resnet(imgR).cuda()\n","\n","\n","\n","\n","def train(model,optimizer, imgL,imgR,disp_gt, resnet):\n","\n","        model.train()\n","        res_fea_L, res_fea_R = extract_resnet_features(resnet, imgL, imgR)\n","\n","        if resnet is None:\n","\n","          imgL   = imgL.cuda()\n","          imgR   = imgR.cuda()\n","        disp_gt = disp_gt.cuda()\n","\n","\n","        #---------\n","        mask = (disp_gt > 0)\n","        mask.detach_()\n","        #----\n","\n","        optimizer.zero_grad()\n","\n","        \n","    \n","        output1, output2, output3 = model(imgL,imgR,res_fea_L, res_fea_R)\n","        output1 = torch.squeeze(output1,1)\n","        output2 = torch.squeeze(output2,1)\n","        output3 = torch.squeeze(output3,1)\n","\n","        loss = 0.5*F.smooth_l1_loss(output1[mask], disp_gt[mask], size_average=True) +0.7*F.smooth_l1_loss(output2[mask], disp_gt[mask], size_average=True) +F.smooth_l1_loss(output3[mask], disp_gt[mask], size_average=True) \n","\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        return loss.item()\n","\n","def validate(model, imgL,imgR,disp_gt,resnet):\n","\n","        model.eval()\n","\n","        res_fea_L, res_fea_R = extract_resnet_features(resnet, imgL, imgR)\n","\n","        if resnet is None:\n","          imgL   = imgL.cuda()\n","          imgR   = imgR.cuda()\n","        \n","\n","        with torch.no_grad():\n","            output3 = model(imgL,imgR, res_fea_L,res_fea_R)\n","\n","        pred_disp = output3.data.cpu().squeeze(1)\n","\n","\n","        return compute_3px_err(pred_disp, disp_gt)\n","        \n","\n","\n","\n","def train_loop(model, trainDataLoader, valDataLoader, optimizer, transfer_learning = False, epochs = 40, save = False):\n","  \n","  least_err = 999999\n","  best_epoch = 0\n","  start_full_time = time.time()\n","  train_loss_per_epoch = []\n","  val_err_per_epoch = []\n","\n","  resnet = None\n","\n","  save_path = root_path + 'finetuned_models/'\n","\n","  if transfer_learning:\n","    \n","    resnet = resnet18(pretrained = True).cpu()\n","      \n","    resnet = nn.Sequential(*list(resnet.children())[:-2])\n","\n","    for name, param in resnet.named_parameters():\n","\n","        param.requires_grad = False\n","\n","  for epoch in range(1, epochs+1):\n","\n","      total_train_loss = 0\n","      total_val_error = 0\n","          \n","      for batch_idx, (imgL_crop, imgR_crop, disp_gt_crop) in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader)):\n","          start_time = time.time()\n","\n","\n","          loss = train(model, optimizer, imgL_crop,imgR_crop, disp_gt_crop,resnet)\n","          \n","          total_train_loss += loss\n","      \n","  \n","\n","      for batch_idx, (imgL, imgR, disp_gt) in  enumerate(valDataLoader):\n","\n","          val_error = validate(model,imgL,imgR, disp_gt,resnet)\n","         \n","          total_val_error += val_error\n","\n","      err = total_val_error/len(valDataLoader)*100\n","      running_loss = total_train_loss/len(trainDataLoader)\n","      train_loss_per_epoch.append(running_loss)\n","      val_err_per_epoch.append(err)\n","\n","      print('epoch %d running training loss = %.3f avg validation 3px-error = %.3f' %(epoch,running_loss ,err))\n","      if err < least_err:\n","          least_err = err\n","          best_epoch = epoch\n","\n","          if save:\n","            \n","          \n","            model_name = 'resnet_finetune_'\n","\n","            savefilename = save_path+model_name+str(epoch)+'.tar'\n","            torch.save({\n","                  'epoch': epoch,\n","                  'state_dict': model.state_dict(),\n","                  'train_loss': running_loss,\n","                  'val_error_3px': err,\n","              }, savefilename)\n","\n","  np.save(save_path+model_name+'train_loss.npy',train_loss_per_epoch)\n","  np.save(save_path+model_name+'val_error.npy',val_err_per_epoch)    \n","\n","  print('full finetune time = %.2f HR' %((time.time() - start_full_time)/3600))\n","  print('Best epoch %d Best Avg error 3px = %.3f' %(best_epoch, least_err))\n"],"metadata":{"id":"o1lWPHYir2WU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_path = root_path + '/dataset/data_stereo_flow/testing/'\n","train_path = root_path + 'dataset/data_stereo_flow/training/'"],"metadata":{"id":"oysIqscQuRoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainDataLoader = torch.utils.data.DataLoader(\n","         KITTI(train_path, mode = 'train'),\n","         batch_size= 4, shuffle= False, drop_last=False)\n","\n","valDataLoader = torch.utils.data.DataLoader(\n","         KITTI(train_path, mode = 'val'), \n","         batch_size= 8, shuffle= False, drop_last=False)\n","\n","testDataLoader = torch.utils.data.DataLoader(\n","         KITTI(train_path, mode = 'test'),\n","         batch_size= 8, shuffle= False, drop_last=False)"],"metadata":{"id":"Wy14zcomuUst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transfer_learning = True\n","model_weights_path = root_path + 'model_weights/pretrained_sceneflow_new.tar'\n","maximum_disp = 192\n","\n","torch.manual_seed(5)\n","\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed(17)\n","model = get_model(maximum_disp,transfer_learning, model_weights_path)\n","optimizer = optim.Adam(model.parameters(), lr=10, betas=(0.9, 0.999))"],"metadata":{"id":"Z1UmkmmjuQVy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683602590457,"user_tz":300,"elapsed":1474,"user":{"displayName":"Darkhan Baizhan","userId":"08245259182187097777"}},"outputId":"77572382-a138-47f9-bf7e-68b9ffb42e7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable parameters in model: 3453376\n"]}]},{"cell_type":"code","source":["train_loop(model, trainDataLoader, valDataLoader, optimizer,transfer_learning,40,False)"],"metadata":{"id":"XXaRTzWrr6Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(model, testDataLoader, resnet):\n","  \n","      for batch_idx, (imgL, imgR, disp_gt) in  enumerate(testDataLoader):\n","\n","          test_error = validate(model,imgL,imgR, disp_gt,resnet)\n","         \n","          total_test_error += test_error\n","\n","      avg_test_err = total_test_error/len(total_test_error)*100\n","\n","      print(f\"average test 3px error: {avg_test_err}\")\n","      return avg_test_err\n","  "],"metadata":{"id":"Vs3ReJbpvSIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(model, testDataLoader, resnet)"],"metadata":{"id":"bE1HrV5Yv0Tf"},"execution_count":null,"outputs":[]}]}